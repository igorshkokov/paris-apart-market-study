{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Headers-and-concept\" data-toc-modified-id=\"Headers-and-concept-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Headers and concept</a></span></li><li><span><a href=\"#Parameters\" data-toc-modified-id=\"Parameters-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Parameters</a></span></li><li><span><a href=\"#Script-to-test\" data-toc-modified-id=\"Script-to-test-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Script to test</a></span></li><li><span><a href=\"#Big-script-to-get-the-data-into-a-file---I-didn't-create-a-function-on-purpose\" data-toc-modified-id=\"Big-script-to-get-the-data-into-a-file---I-didn't-create-a-function-on-purpose-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Big script to get the data into a file - I didn't create a function on purpose</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#ParisSud\" data-toc-modified-id=\"ParisSud-4.0.1\"><span class=\"toc-item-num\">4.0.1&nbsp;&nbsp;</span>ParisSud</a></span></li><li><span><a href=\"#ParisOuest\" data-toc-modified-id=\"ParisOuest-4.0.2\"><span class=\"toc-item-num\">4.0.2&nbsp;&nbsp;</span>ParisOuest</a></span></li><li><span><a href=\"#ParisNord\" data-toc-modified-id=\"ParisNord-4.0.3\"><span class=\"toc-item-num\">4.0.3&nbsp;&nbsp;</span>ParisNord</a></span></li><li><span><a href=\"#ParisCentre\" data-toc-modified-id=\"ParisCentre-4.0.4\"><span class=\"toc-item-num\">4.0.4&nbsp;&nbsp;</span>ParisCentre</a></span></li><li><span><a href=\"#ParisEst\" data-toc-modified-id=\"ParisEst-4.0.5\"><span class=\"toc-item-num\">4.0.5&nbsp;&nbsp;</span>ParisEst</a></span></li></ul></li></ul></li><li><span><a href=\"#Dataset\" data-toc-modified-id=\"Dataset-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#after\" data-toc-modified-id=\"after-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>after</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as r\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "import warnings\n",
    "warnings.simplefilter('ignore') #ignore the warnings, not the errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Headers and concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers=\"\"\"Accept: */*\n",
    "Accept-Encoding: gzip, deflate, br\n",
    "Accept-Language: it-IT,it;q=0.9,en-US;q=0.8,en;q=0.7\n",
    "Cache-Control: no-cache\n",
    "Connection: keep-alive\n",
    "Cookie: lang=fr; acceptcookies=1; cnil=1; atidvisitor=%7B%22name%22%3A%22atidvisitor%22%2C%22val%22%3A%7B%22vrn%22%3A%22-590496-%22%7D%2C%22options%22%3A%7B%22path%22%3A%22%2F%22%2C%22session%22%3A15724800%2C%22end%22%3A15724800%7D%7D; _uetsid=f16d650012a111ebb27669ee32d7f0b4; _uetvid=f16dd0a012a111ebbb337d2c7148adc9; _ga=GA1.2.185254921.1603177231; _gid=GA1.2.879719661.1603177231; _fbp=fb.1.1603177232895.1293203901; _gat=1; derniere_recherche=%7B%22produit%22%3A%22location%22%2C%22nb_pieces%22%3A%7B%22min%22%3Anull%2C%22max%22%3Anull%7D%2C%22nb_chambres%22%3A%7B%22min%22%3Anull%2C%22max%22%3Anull%7D%2C%22prix%22%3A%7B%22min%22%3Anull%2C%22max%22%3A1200%7D%2C%22surface%22%3A%7B%22min%22%3Anull%2C%22max%22%3Anull%7D%2C%22surface_terrain%22%3A%7B%22min%22%3Anull%2C%22max%22%3Anull%7D%2C%22geo_objets%22%3A%5B37778%5D%7D\n",
    "Host: www.pap.fr\n",
    "Pragma: no-cache\n",
    "Referer: https://www.pap.fr/proximite/location-immobilier-particulier-paris-11e-g37778-jusqu-a-1200-euros-10\n",
    "Sec-Fetch-Dest: empty\n",
    "Sec-Fetch-Mode: cors\n",
    "Sec-Fetch-Site: same-origin\n",
    "User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36\n",
    "X-Requested-With: XMLHttpRequest\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers=dict([i.strip().split(': ') for i in headers.split('\\n')])\n",
    "url='https://www.pap.fr/proximite/immobilier-location-appartement-paris-11e-g37778-jusqu-a-1200-euros-3'\n",
    "html=r.get(url,headers=headers).content\n",
    "soup=BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "price=[int(''.join(i.text.replace('.', '').split()[:-1])) for i in soup.select('.item-price')]\n",
    "\n",
    "surface_m2=[int(i) for i in re.findall(r'li\\>([0-9]+) \\<small\\>', str(soup.select('.item-tags')))]\n",
    "\n",
    "rooms=[int(i.strip()[0]) for i in re.findall(r'\\d [p]', str(soup.select('.item-tags')))]\n",
    "\n",
    "links=[str('https://www.pap.fr' + i.get('href')) for i in soup.select('.item-title')]\n",
    "\n",
    "district=[i.text.strip()[6:8] for i in soup.select('.h1')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '3E',\n",
       " '3E',\n",
       " '3E',\n",
       " '3E',\n",
       " '3E',\n",
       " '3E',\n",
       " '3E',\n",
       " '3E',\n",
       " '3E',\n",
       " '3E',\n",
       " '3E']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "district=[i.text.strip()[6:8] for i in soup.select('.h1')]\n",
    "district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>N_rooms</th>\n",
       "      <th>Surface_m2</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1085</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>https://www.pap.fr/annonces/appartement-paris-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1300</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>https://www.pap.fr/annonces/appartement-paris-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1295</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>https://www.pap.fr/annonces/appartement-paris-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1150</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>https://www.pap.fr/annonces/appartement-paris-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1150</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>https://www.pap.fr/annonces/appartement-paris-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Price  N_rooms  Surface_m2  \\\n",
       "0   1085        1          20   \n",
       "1   1300        2          34   \n",
       "2   1295        2          24   \n",
       "3   1150        2          24   \n",
       "4   1150        1          25   \n",
       "\n",
       "                                                Link  \n",
       "0  https://www.pap.fr/annonces/appartement-paris-...  \n",
       "1  https://www.pap.fr/annonces/appartement-paris-...  \n",
       "2  https://www.pap.fr/annonces/appartement-paris-...  \n",
       "3  https://www.pap.fr/annonces/appartement-paris-...  \n",
       "4  https://www.pap.fr/annonces/appartement-paris-...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(price)\n",
    "df.columns=['Price']\n",
    "df['N_rooms'] = rooms\n",
    "df['Surface_m2'] = surface_m2\n",
    "df['Link'] = links\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 12-15 - ParisSud\n",
    "zone=str('g37779g37780g37781g37782')\n",
    "sort_list=['12','13','14','15']\n",
    "zone_name='ParisSud'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 7-8-16 - ParisOuest\n",
    "zone=str('g37774g37775g37783')\n",
    "sort_list=['7E','8E','16']\n",
    "zone_name='ParisOuest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 17-20 - ParisNord\n",
    "zone=str('g37784g37785g37786g37787')\n",
    "sort_list=['17','18','19','20']\n",
    "zone_name='ParisNord'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 1-6 - ParisCentre\n",
    "zone=str('g37768g37769g37770g37771g37772g37773')\n",
    "sort_list=['1E','2E','3E','4E', '5E','6E']\n",
    "zone_name='ParisCentre'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 9-11 - ParisEst\n",
    "zone=str('g37776g37777g37778')\n",
    "sort_list=['9E','10','11']\n",
    "zone_name='ParisEst'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "price_limit=str(1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pages=30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Script to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can expect to get the results for 30 pages\n",
      "Getting values from page 1 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 1, continuing to the next page...\n",
      "Getting values from page 2 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 2, continuing to the next page...\n",
      "Getting values from page 3 for g37768g37769g37770g37771g37772g37773...\n",
      "Getting values from page 4 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 4, continuing to the next page...\n",
      "Getting values from page 5 for g37768g37769g37770g37771g37772g37773...\n",
      "Getting values from page 6 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 6, continuing to the next page...\n",
      "Getting values from page 7 for g37768g37769g37770g37771g37772g37773...\n",
      "Getting values from page 8 for g37768g37769g37770g37771g37772g37773...\n",
      "Getting values from page 9 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 9, continuing to the next page...\n",
      "Getting values from page 10 for g37768g37769g37770g37771g37772g37773...\n",
      "Getting values from page 11 for g37768g37769g37770g37771g37772g37773...\n",
      "Getting values from page 12 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 12, continuing to the next page...\n",
      "Getting values from page 13 for g37768g37769g37770g37771g37772g37773...\n",
      "Getting values from page 14 for g37768g37769g37770g37771g37772g37773...\n",
      "Getting values from page 15 for g37768g37769g37770g37771g37772g37773...\n",
      "Getting values from page 16 for g37768g37769g37770g37771g37772g37773...\n",
      "Getting values from page 17 for g37768g37769g37770g37771g37772g37773...\n",
      "Getting values from page 18 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 18, continuing to the next page...\n",
      "Getting values from page 19 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 19, continuing to the next page...\n",
      "Getting values from page 20 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 20, continuing to the next page...\n",
      "Getting values from page 21 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 21, continuing to the next page...\n",
      "Getting values from page 22 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 22, continuing to the next page...\n",
      "Getting values from page 23 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 23, continuing to the next page...\n",
      "Getting values from page 24 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 24, continuing to the next page...\n",
      "Getting values from page 25 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 25, continuing to the next page...\n",
      "Getting values from page 26 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 26, continuing to the next page...\n",
      "Getting values from page 27 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 27, continuing to the next page...\n",
      "Getting values from page 28 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 28, continuing to the next page...\n",
      "Getting values from page 29 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 29, continuing to the next page...\n",
      "Getting values from page 30 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 30, continuing to the next page...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# create the dataset and get pages number\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# notify user about pages quantity\n",
    "    \n",
    "print('You can expect to get the results for ' + str(pages) + ' pages')\n",
    "\n",
    "# loop over every page\n",
    "\n",
    "for k in range(1, pages+1):\n",
    "    try:\n",
    "\n",
    "        # get the soup from a link\n",
    "\n",
    "        print('Getting values from page '+str(k)+' for '+zone+'...')\n",
    "            \n",
    "        url='https://www.pap.fr/proximite/immobilier-location-appartement-paris-11e-'+zone+'-jusqu-a-'+price_limit+'-euros-'+str(k)\n",
    "    \n",
    "        html=r.get(url,headers=headers).content\n",
    "        time.sleep(1)\n",
    "        soup=BeautifulSoup(html)\n",
    "\n",
    "        # get the info for every column\n",
    "\n",
    "        price=[int(''.join(i.text.replace('.', '').split()[:-1])) for i in soup.select('.item-price')]\n",
    "        surface_m2=[int(i) for i in re.findall(r'li\\>([0-9]+) \\<small\\>', str(soup.select('.item-tags')))]\n",
    "        rooms=[int(i.strip()[0]) for i in re.findall(r'\\d [p]', str(soup.select('.item-tags')))]\n",
    "        links=[str('https://www.pap.fr' + i.get('href')) for i in soup.select('.item-title')]\n",
    "        district=[i.text.strip()[6:8] for i in soup.select('.h1')]\n",
    "\n",
    "        # append to the dataset\n",
    "\n",
    "        df_local=pd.DataFrame(price)\n",
    "        df_local.columns=['Price']\n",
    "        df_local['N_rooms'] = rooms\n",
    "        df_local['Surface_m2'] = surface_m2\n",
    "        df_local['Link'] = links\n",
    "        df_local['District'] = district\n",
    "        df_local['Zone'] = str(zone_name)\n",
    "        \n",
    "        df=df.append(df_local, ignore_index = True) \n",
    "\n",
    "        # two seconds - not to get banned\n",
    "\n",
    "#         time.sleep(1)\n",
    "\n",
    "        # in case of an error - skip the page:\n",
    "\n",
    "    except:\n",
    "        print(f'Encountered a problem on page {k}, continuing to the next page...')\n",
    "        continue\n",
    "\n",
    "    # create new columns and print Done\n",
    "\n",
    "df['Price_per_m2'] = df.Price / df.Surface_m2     \n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By sorting out only for needed districts, the dataset will lose 58 out of 148 rows.\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "print('By sorting out only for needed districts, the dataset will lose ' + str(df.shape[0]-df.loc[df.District.isin(sort_list)].shape[0]) + ' out of ' + str(df.shape[0]) + ' rows.')\n",
    "\n",
    "df= df.loc[df.District.isin(sort_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big script to get the data into a file - I didn't create a function on purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "\n",
    "price_limit=str(1200)\n",
    "pages=30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ParisSud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "# 12-15 - ParisSud\n",
    "zone=str('g37779g37780g37781g37782')\n",
    "sort_list=['12','13','14','15']\n",
    "zone_name='ParisSud'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize df\n",
    "\n",
    "df1 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can expect to get the results for 30 pages\n",
      "Getting values from page 1 for g37779g37780g37781g37782...\n",
      "Getting values from page 2 for g37779g37780g37781g37782...\n",
      "Getting values from page 3 for g37779g37780g37781g37782...\n",
      "Getting values from page 4 for g37779g37780g37781g37782...\n",
      "Getting values from page 5 for g37779g37780g37781g37782...\n",
      "Getting values from page 6 for g37779g37780g37781g37782...\n",
      "Encountered a problem on page 6, continuing to the next page...\n",
      "Getting values from page 7 for g37779g37780g37781g37782...\n",
      "Getting values from page 8 for g37779g37780g37781g37782...\n",
      "Getting values from page 9 for g37779g37780g37781g37782...\n",
      "Getting values from page 10 for g37779g37780g37781g37782...\n",
      "Getting values from page 11 for g37779g37780g37781g37782...\n",
      "Encountered a problem on page 11, continuing to the next page...\n",
      "Getting values from page 12 for g37779g37780g37781g37782...\n",
      "Encountered a problem on page 12, continuing to the next page...\n",
      "Getting values from page 13 for g37779g37780g37781g37782...\n",
      "Getting values from page 14 for g37779g37780g37781g37782...\n",
      "Getting values from page 15 for g37779g37780g37781g37782...\n",
      "Getting values from page 16 for g37779g37780g37781g37782...\n",
      "Encountered a problem on page 16, continuing to the next page...\n",
      "Getting values from page 17 for g37779g37780g37781g37782...\n",
      "Encountered a problem on page 17, continuing to the next page...\n",
      "Getting values from page 18 for g37779g37780g37781g37782...\n",
      "Getting values from page 19 for g37779g37780g37781g37782...\n",
      "Getting values from page 20 for g37779g37780g37781g37782...\n",
      "Getting values from page 21 for g37779g37780g37781g37782...\n",
      "Getting values from page 22 for g37779g37780g37781g37782...\n",
      "Getting values from page 23 for g37779g37780g37781g37782...\n",
      "Getting values from page 24 for g37779g37780g37781g37782...\n",
      "Getting values from page 25 for g37779g37780g37781g37782...\n",
      "Getting values from page 26 for g37779g37780g37781g37782...\n",
      "Getting values from page 27 for g37779g37780g37781g37782...\n",
      "Getting values from page 28 for g37779g37780g37781g37782...\n",
      "Getting values from page 29 for g37779g37780g37781g37782...\n",
      "Getting values from page 30 for g37779g37780g37781g37782...\n",
      "By filtering out only for needed districts, the dataset will lose 96 out of 375 rows.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# notify user about pages quantity\n",
    "    \n",
    "print('You can expect to get the results for ' + str(pages) + ' pages')\n",
    "\n",
    "# loop over every page\n",
    "\n",
    "for k in range(1, pages+1):\n",
    "    try:\n",
    "\n",
    "        # get the soup from a link\n",
    "\n",
    "        print('Getting values from page '+str(k)+' for '+zone+'...')\n",
    "            \n",
    "        url='https://www.pap.fr/proximite/immobilier-location-appartement-paris-11e-'+zone+'-jusqu-a-'+price_limit+'-euros-'+str(k)\n",
    "    \n",
    "        html=r.get(url,headers=headers).content\n",
    "        soup=BeautifulSoup(html)\n",
    "\n",
    "        # get the info for every column\n",
    "\n",
    "        price=[int(''.join(i.text.replace('.', '').split()[:-1])) for i in soup.select('.item-price')]\n",
    "        surface_m2=[int(i) for i in re.findall(r'li\\>([0-9]+) \\<small\\>', str(soup.select('.item-tags')))]\n",
    "        rooms=[int(i.strip()[0]) for i in re.findall(r'\\d [p]', str(soup.select('.item-tags')))]\n",
    "        links=[str('https://www.pap.fr' + i.get('href')) for i in soup.select('.item-title')]\n",
    "        district=[i.text.strip()[6:8] for i in soup.select('.h1')]\n",
    "\n",
    "        # append to the dataset\n",
    "\n",
    "        df_local=pd.DataFrame(price)\n",
    "        df_local.columns=['Price']\n",
    "        df_local['N_rooms'] = rooms\n",
    "        df_local['Surface_m2'] = surface_m2\n",
    "        df_local['Link'] = links\n",
    "        df_local['District'] = district\n",
    "        df_local['Zone'] = str(zone_name)\n",
    "                \n",
    "        df1=df1.append(df_local, ignore_index = True) \n",
    "\n",
    "        # two seconds - not to get banned\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        # in case of an error - skip the page:\n",
    "\n",
    "    except:\n",
    "        print(f'Encountered a problem on page {k}, continuing to the next page...')\n",
    "        continue\n",
    "\n",
    "    # create new columns and print Done\n",
    "\n",
    "df1['Price_per_m2'] = df1.Price / df1.Surface_m2    \n",
    "\n",
    "    # drop dublicates and sort only by needed districts\n",
    "    \n",
    "print('By filtering out only for needed districts, the dataset will lose ' + str(df1.shape[0]-df1.loc[df1.District.isin(sort_list)].shape[0]) + ' out of ' + str(df1.shape[0]) + ' rows.')\n",
    "df1=df1.loc[df1.District.isin(sort_list)]\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ParisOuest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-8-16 - ParisOuest\n",
    "zone=str('g37774g37775g37783')\n",
    "sort_list=['7E','8E','16']\n",
    "zone_name='ParisOuest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize df\n",
    "\n",
    "df2 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can expect to get the results for 30 pages\n",
      "Getting values from page 1 for g37774g37775g37783...\n",
      "Encountered a problem on page 1, continuing to the next page...\n",
      "Getting values from page 2 for g37774g37775g37783...\n",
      "Getting values from page 3 for g37774g37775g37783...\n",
      "Getting values from page 4 for g37774g37775g37783...\n",
      "Encountered a problem on page 4, continuing to the next page...\n",
      "Getting values from page 5 for g37774g37775g37783...\n",
      "Getting values from page 6 for g37774g37775g37783...\n",
      "Getting values from page 7 for g37774g37775g37783...\n",
      "Getting values from page 8 for g37774g37775g37783...\n",
      "Encountered a problem on page 8, continuing to the next page...\n",
      "Getting values from page 9 for g37774g37775g37783...\n",
      "Encountered a problem on page 9, continuing to the next page...\n",
      "Getting values from page 10 for g37774g37775g37783...\n",
      "Encountered a problem on page 10, continuing to the next page...\n",
      "Getting values from page 11 for g37774g37775g37783...\n",
      "Encountered a problem on page 11, continuing to the next page...\n",
      "Getting values from page 12 for g37774g37775g37783...\n",
      "Encountered a problem on page 12, continuing to the next page...\n",
      "Getting values from page 13 for g37774g37775g37783...\n",
      "Getting values from page 14 for g37774g37775g37783...\n",
      "Getting values from page 15 for g37774g37775g37783...\n",
      "Getting values from page 16 for g37774g37775g37783...\n",
      "Getting values from page 17 for g37774g37775g37783...\n",
      "Encountered a problem on page 17, continuing to the next page...\n",
      "Getting values from page 18 for g37774g37775g37783...\n",
      "Encountered a problem on page 18, continuing to the next page...\n",
      "Getting values from page 19 for g37774g37775g37783...\n",
      "Encountered a problem on page 19, continuing to the next page...\n",
      "Getting values from page 20 for g37774g37775g37783...\n",
      "Encountered a problem on page 20, continuing to the next page...\n",
      "Getting values from page 21 for g37774g37775g37783...\n",
      "Encountered a problem on page 21, continuing to the next page...\n",
      "Getting values from page 22 for g37774g37775g37783...\n",
      "Encountered a problem on page 22, continuing to the next page...\n",
      "Getting values from page 23 for g37774g37775g37783...\n",
      "Encountered a problem on page 23, continuing to the next page...\n",
      "Getting values from page 24 for g37774g37775g37783...\n",
      "Encountered a problem on page 24, continuing to the next page...\n",
      "Getting values from page 25 for g37774g37775g37783...\n",
      "Encountered a problem on page 25, continuing to the next page...\n",
      "Getting values from page 26 for g37774g37775g37783...\n",
      "Encountered a problem on page 26, continuing to the next page...\n",
      "Getting values from page 27 for g37774g37775g37783...\n",
      "Encountered a problem on page 27, continuing to the next page...\n",
      "Getting values from page 28 for g37774g37775g37783...\n",
      "Encountered a problem on page 28, continuing to the next page...\n",
      "Getting values from page 29 for g37774g37775g37783...\n",
      "Encountered a problem on page 29, continuing to the next page...\n",
      "Getting values from page 30 for g37774g37775g37783...\n",
      "Encountered a problem on page 30, continuing to the next page...\n",
      "By sorting out only for needed districts, the dataset will lose 59 out of 135 rows.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# notify user about pages quantity\n",
    "    \n",
    "print('You can expect to get the results for ' + str(pages) + ' pages')\n",
    "\n",
    "# loop over every page\n",
    "\n",
    "for k in range(1, pages+1):\n",
    "    try:\n",
    "\n",
    "        # get the soup from a link\n",
    "\n",
    "        print('Getting values from page '+str(k)+' for '+zone+'...')\n",
    "            \n",
    "        url='https://www.pap.fr/proximite/immobilier-location-appartement-paris-11e-'+zone+'-jusqu-a-'+price_limit+'-euros-'+str(k)\n",
    "    \n",
    "        html=r.get(url,headers=headers).content\n",
    "        soup=BeautifulSoup(html)\n",
    "\n",
    "        # get the info for every column\n",
    "\n",
    "        price=[int(''.join(i.text.replace('.', '').split()[:-1])) for i in soup.select('.item-price')]\n",
    "        surface_m2=[int(i) for i in re.findall(r'li\\>([0-9]+) \\<small\\>', str(soup.select('.item-tags')))]\n",
    "        rooms=[int(i.strip()[0]) for i in re.findall(r'\\d [p]', str(soup.select('.item-tags')))]\n",
    "        links=[str('https://www.pap.fr' + i.get('href')) for i in soup.select('.item-title')]\n",
    "        district=[i.text.strip()[6:8] for i in soup.select('.h1')]\n",
    "\n",
    "        # append to the dataset\n",
    "\n",
    "        df_local=pd.DataFrame(price)\n",
    "        df_local.columns=['Price']\n",
    "        df_local['N_rooms'] = rooms\n",
    "        df_local['Surface_m2'] = surface_m2\n",
    "        df_local['Link'] = links\n",
    "        df_local['District'] = district\n",
    "        df_local['Zone'] = str(zone_name)\n",
    "        \n",
    "        df2=df2.append(df_local, ignore_index = True) \n",
    "\n",
    "        # two seconds - not to get banned\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        # in case of an error - skip the page:\n",
    "\n",
    "    except:\n",
    "        print(f'Encountered a problem on page {k}, continuing to the next page...')\n",
    "        continue\n",
    "\n",
    "    # create new columns and print Done\n",
    "\n",
    "df2['Price_per_m2'] = df2.Price / df2.Surface_m2    \n",
    "\n",
    "    # drop dublicates and sort only by needed districts\n",
    "    \n",
    "print('By sorting out only for needed districts, the dataset will lose ' + str(df2.shape[0]-df2.loc[df2.District.isin(sort_list)].shape[0]) + ' out of ' + str(df2.shape[0]) + ' rows.')\n",
    "df2=df2.loc[df2.District.isin(sort_list)]\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ParisNord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17-20 - ParisNord\n",
    "zone=str('g37784g37785g37786g37787')\n",
    "sort_list=['17','18','19','20']\n",
    "zone_name='ParisNord'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize df\n",
    "\n",
    "df3 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can expect to get the results for 30 pages\n",
      "Getting values from page 1 for g37784g37785g37786g37787...\n",
      "Getting values from page 2 for g37784g37785g37786g37787...\n",
      "Getting values from page 3 for g37784g37785g37786g37787...\n",
      "Getting values from page 4 for g37784g37785g37786g37787...\n",
      "Getting values from page 5 for g37784g37785g37786g37787...\n",
      "Encountered a problem on page 5, continuing to the next page...\n",
      "Getting values from page 6 for g37784g37785g37786g37787...\n",
      "Getting values from page 7 for g37784g37785g37786g37787...\n",
      "Getting values from page 8 for g37784g37785g37786g37787...\n",
      "Getting values from page 9 for g37784g37785g37786g37787...\n",
      "Encountered a problem on page 9, continuing to the next page...\n",
      "Getting values from page 10 for g37784g37785g37786g37787...\n",
      "Getting values from page 11 for g37784g37785g37786g37787...\n",
      "Getting values from page 12 for g37784g37785g37786g37787...\n",
      "Getting values from page 13 for g37784g37785g37786g37787...\n",
      "Getting values from page 14 for g37784g37785g37786g37787...\n",
      "Encountered a problem on page 14, continuing to the next page...\n",
      "Getting values from page 15 for g37784g37785g37786g37787...\n",
      "Getting values from page 16 for g37784g37785g37786g37787...\n",
      "Encountered a problem on page 16, continuing to the next page...\n",
      "Getting values from page 17 for g37784g37785g37786g37787...\n",
      "Encountered a problem on page 17, continuing to the next page...\n",
      "Getting values from page 18 for g37784g37785g37786g37787...\n",
      "Getting values from page 19 for g37784g37785g37786g37787...\n",
      "Getting values from page 20 for g37784g37785g37786g37787...\n",
      "Getting values from page 21 for g37784g37785g37786g37787...\n",
      "Getting values from page 22 for g37784g37785g37786g37787...\n",
      "Getting values from page 23 for g37784g37785g37786g37787...\n",
      "Getting values from page 24 for g37784g37785g37786g37787...\n",
      "Getting values from page 25 for g37784g37785g37786g37787...\n",
      "Getting values from page 26 for g37784g37785g37786g37787...\n",
      "Getting values from page 27 for g37784g37785g37786g37787...\n",
      "Getting values from page 28 for g37784g37785g37786g37787...\n",
      "Getting values from page 29 for g37784g37785g37786g37787...\n",
      "Getting values from page 30 for g37784g37785g37786g37787...\n",
      "By sorting out only for needed districts, the dataset will lose 17 out of 149 rows.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# notify user about pages quantity\n",
    "    \n",
    "print('You can expect to get the results for ' + str(pages) + ' pages')\n",
    "\n",
    "# loop over every page\n",
    "\n",
    "for k in range(1, pages+1):\n",
    "    try:\n",
    "\n",
    "        # get the soup from a link\n",
    "\n",
    "        print('Getting values from page '+str(k)+' for '+zone+'...')\n",
    "            \n",
    "        url='https://www.pap.fr/proximite/immobilier-location-appartement-paris-17e-'+zone+'-jusqu-a-'+price_limit+'-euros-'+str(k)\n",
    "    \n",
    "        html=r.get(url,headers=headers).content\n",
    "        soup=BeautifulSoup(html)\n",
    "\n",
    "        # get the info for every column\n",
    "\n",
    "        price=[int(''.join(i.text.replace('.', '').split()[:-1])) for i in soup.select('.item-price')]\n",
    "        surface_m2=[int(i) for i in re.findall(r'li\\>([0-9]+) \\<small\\>', str(soup.select('.item-tags')))]\n",
    "        rooms=[int(i.strip()[0]) for i in re.findall(r'\\d [p]', str(soup.select('.item-tags')))]\n",
    "        links=[str('https://www.pap.fr' + i.get('href')) for i in soup.select('.item-title')]\n",
    "        district=[i.text.strip()[6:8] for i in soup.select('.h1')]\n",
    "\n",
    "        # append to the dataset\n",
    "\n",
    "        df_local=pd.DataFrame(price)\n",
    "        df_local.columns=['Price']\n",
    "        df_local['N_rooms'] = rooms\n",
    "        df_local['Surface_m2'] = surface_m2\n",
    "        df_local['Link'] = links\n",
    "        df_local['District'] = district\n",
    "        df_local['Zone'] = str(zone_name)\n",
    "        \n",
    "        df3=df3.append(df_local, ignore_index = True) \n",
    "\n",
    "        # two seconds - not to get banned\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        # in case of an error - skip the page:\n",
    "\n",
    "    except:\n",
    "        print(f'Encountered a problem on page {k}, continuing to the next page...')\n",
    "        continue\n",
    "\n",
    "    # create new columns and print Done\n",
    "\n",
    "df3['Price_per_m2'] = df3.Price / df3.Surface_m2    \n",
    "\n",
    "    # drop dublicates and sort only by needed districts\n",
    "    \n",
    "df3.drop_duplicates(inplace=True)\n",
    "print('By sorting out only for needed districts, the dataset will lose ' + str(df3.shape[0]-df3.loc[df3.District.isin(sort_list)].shape[0]) + ' out of ' + str(df3.shape[0]) + ' rows.')\n",
    "df3=df3.loc[df3.District.isin(sort_list)]\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ParisCentre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-6 - ParisCentre\n",
    "zone=str('g37768g37769g37770g37771g37772g37773')\n",
    "sort_list=['1E','2E','3E','4E', '5E','6E']\n",
    "zone_name='ParisCentre'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize df\n",
    "\n",
    "df4 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can expect to get the results for 30 pages\n",
      "Getting values from page 1 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 1, continuing to the next page...\n",
      "Getting values from page 2 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 2, continuing to the next page...\n",
      "Getting values from page 3 for g37768g37769g37770g37771g37772g37773...\n",
      "Getting values from page 4 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 4, continuing to the next page...\n",
      "Getting values from page 5 for g37768g37769g37770g37771g37772g37773...\n",
      "Getting values from page 6 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 6, continuing to the next page...\n",
      "Getting values from page 7 for g37768g37769g37770g37771g37772g37773...\n",
      "Getting values from page 8 for g37768g37769g37770g37771g37772g37773...\n",
      "Getting values from page 9 for g37768g37769g37770g37771g37772g37773...\n",
      "Getting values from page 10 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 10, continuing to the next page...\n",
      "Getting values from page 11 for g37768g37769g37770g37771g37772g37773...\n",
      "Getting values from page 12 for g37768g37769g37770g37771g37772g37773...\n",
      "Getting values from page 13 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 13, continuing to the next page...\n",
      "Getting values from page 14 for g37768g37769g37770g37771g37772g37773...\n",
      "Getting values from page 15 for g37768g37769g37770g37771g37772g37773...\n",
      "Getting values from page 16 for g37768g37769g37770g37771g37772g37773...\n",
      "Getting values from page 17 for g37768g37769g37770g37771g37772g37773...\n",
      "Getting values from page 18 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 18, continuing to the next page...\n",
      "Getting values from page 19 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 19, continuing to the next page...\n",
      "Getting values from page 20 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 20, continuing to the next page...\n",
      "Getting values from page 21 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 21, continuing to the next page...\n",
      "Getting values from page 22 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 22, continuing to the next page...\n",
      "Getting values from page 23 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 23, continuing to the next page...\n",
      "Getting values from page 24 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 24, continuing to the next page...\n",
      "Getting values from page 25 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 25, continuing to the next page...\n",
      "Getting values from page 26 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 26, continuing to the next page...\n",
      "Getting values from page 27 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 27, continuing to the next page...\n",
      "Getting values from page 28 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 28, continuing to the next page...\n",
      "Getting values from page 29 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 29, continuing to the next page...\n",
      "Getting values from page 30 for g37768g37769g37770g37771g37772g37773...\n",
      "Encountered a problem on page 30, continuing to the next page...\n",
      "By sorting out only for needed districts, the dataset will lose 110 out of 165 rows.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# notify user about pages quantity\n",
    "    \n",
    "print('You can expect to get the results for ' + str(pages) + ' pages')\n",
    "\n",
    "# loop over every page\n",
    "\n",
    "for k in range(1, pages+1):\n",
    "    try:\n",
    "\n",
    "        # get the soup from a link\n",
    "\n",
    "        print('Getting values from page '+str(k)+' for '+zone+'...')\n",
    "            \n",
    "        url='https://www.pap.fr/proximite/immobilier-location-appartement-paris-1er-'+zone+'-jusqu-a-'+price_limit+'-euros-'+str(k)\n",
    "    \n",
    "        html=r.get(url,headers=headers).content\n",
    "        soup=BeautifulSoup(html)\n",
    "\n",
    "        # get the info for every column\n",
    "\n",
    "        price=[int(''.join(i.text.replace('.', '').split()[:-1])) for i in soup.select('.item-price')]\n",
    "        surface_m2=[int(i) for i in re.findall(r'li\\>([0-9]+) \\<small\\>', str(soup.select('.item-tags')))]\n",
    "        rooms=[int(i.strip()[0]) for i in re.findall(r'\\d [p]', str(soup.select('.item-tags')))]\n",
    "        links=[str('https://www.pap.fr' + i.get('href')) for i in soup.select('.item-title')]\n",
    "        district=[i.text.strip()[6:8] for i in soup.select('.h1')]\n",
    "\n",
    "        # append to the dataset\n",
    "\n",
    "        df_local=pd.DataFrame(price)\n",
    "        df_local.columns=['Price']\n",
    "        df_local['N_rooms'] = rooms\n",
    "        df_local['Surface_m2'] = surface_m2\n",
    "        df_local['Link'] = links\n",
    "        df_local['District'] = district\n",
    "        df_local['Zone'] = str(zone_name)\n",
    "        \n",
    "        df4=df4.append(df_local, ignore_index = True) \n",
    "\n",
    "        # two seconds - not to get banned\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        # in case of an error - skip the page:\n",
    "\n",
    "    except:\n",
    "        print(f'Encountered a problem on page {k}, continuing to the next page...')\n",
    "        continue\n",
    "\n",
    "    # create new columns and print Done\n",
    "\n",
    "df4['Price_per_m2'] = df4.Price / df4.Surface_m2    \n",
    "\n",
    "    # drop dublicates and sort only by needed districts\n",
    "    \n",
    "print('By sorting out only for needed districts, the dataset will lose ' + str(df4.shape[0]-df4.loc[df4.District.isin(sort_list)].shape[0]) + ' out of ' + str(df4.shape[0]) + ' rows.')\n",
    "df4=df4.loc[df4.District.isin(sort_list)]\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ParisEst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-11 - ParisEst\n",
    "zone=str('g37776g37777g37778')\n",
    "sort_list=['9E','10','11']\n",
    "zone_name='ParisEst'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize df\n",
    "\n",
    "df5 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can expect to get the results for 30 pages\n",
      "Getting values from page 1 for g37776g37777g37778...\n",
      "Encountered a problem on page 1, continuing to the next page...\n",
      "Getting values from page 2 for g37776g37777g37778...\n",
      "Getting values from page 3 for g37776g37777g37778...\n",
      "Getting values from page 4 for g37776g37777g37778...\n",
      "Getting values from page 5 for g37776g37777g37778...\n",
      "Encountered a problem on page 5, continuing to the next page...\n",
      "Getting values from page 6 for g37776g37777g37778...\n",
      "Encountered a problem on page 6, continuing to the next page...\n",
      "Getting values from page 7 for g37776g37777g37778...\n",
      "Encountered a problem on page 7, continuing to the next page...\n",
      "Getting values from page 8 for g37776g37777g37778...\n",
      "Getting values from page 9 for g37776g37777g37778...\n",
      "Getting values from page 10 for g37776g37777g37778...\n",
      "Getting values from page 11 for g37776g37777g37778...\n",
      "Getting values from page 12 for g37776g37777g37778...\n",
      "Getting values from page 13 for g37776g37777g37778...\n",
      "Encountered a problem on page 13, continuing to the next page...\n",
      "Getting values from page 14 for g37776g37777g37778...\n",
      "Getting values from page 15 for g37776g37777g37778...\n",
      "Getting values from page 16 for g37776g37777g37778...\n",
      "Getting values from page 17 for g37776g37777g37778...\n",
      "Encountered a problem on page 17, continuing to the next page...\n",
      "Getting values from page 18 for g37776g37777g37778...\n",
      "Encountered a problem on page 18, continuing to the next page...\n",
      "Getting values from page 19 for g37776g37777g37778...\n",
      "Encountered a problem on page 19, continuing to the next page...\n",
      "Getting values from page 20 for g37776g37777g37778...\n",
      "Encountered a problem on page 20, continuing to the next page...\n",
      "Getting values from page 21 for g37776g37777g37778...\n",
      "Encountered a problem on page 21, continuing to the next page...\n",
      "Getting values from page 22 for g37776g37777g37778...\n",
      "Encountered a problem on page 22, continuing to the next page...\n",
      "Getting values from page 23 for g37776g37777g37778...\n",
      "Encountered a problem on page 23, continuing to the next page...\n",
      "Getting values from page 24 for g37776g37777g37778...\n",
      "Encountered a problem on page 24, continuing to the next page...\n",
      "Getting values from page 25 for g37776g37777g37778...\n",
      "Encountered a problem on page 25, continuing to the next page...\n",
      "Getting values from page 26 for g37776g37777g37778...\n",
      "Encountered a problem on page 26, continuing to the next page...\n",
      "Getting values from page 27 for g37776g37777g37778...\n",
      "Encountered a problem on page 27, continuing to the next page...\n",
      "Getting values from page 28 for g37776g37777g37778...\n",
      "Encountered a problem on page 28, continuing to the next page...\n",
      "Getting values from page 29 for g37776g37777g37778...\n",
      "Encountered a problem on page 29, continuing to the next page...\n",
      "Getting values from page 30 for g37776g37777g37778...\n",
      "Encountered a problem on page 30, continuing to the next page...\n",
      "By sorting out only for needed districts, the dataset will lose 98 out of 165 rows.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# notify user about pages quantity\n",
    "    \n",
    "print('You can expect to get the results for ' + str(pages) + ' pages')\n",
    "\n",
    "# loop over every page\n",
    "\n",
    "for k in range(1, pages+1):\n",
    "    try:\n",
    "\n",
    "        # get the soup from a link\n",
    "\n",
    "        print('Getting values from page '+str(k)+' for '+zone+'...')\n",
    "            \n",
    "        url='https://www.pap.fr/proximite/immobilier-location-appartement-paris-11e-'+zone+'-jusqu-a-'+price_limit+'-euros-'+str(k)\n",
    "    \n",
    "        html=r.get(url,headers=headers).content\n",
    "        soup=BeautifulSoup(html)\n",
    "\n",
    "        # get the info for every column\n",
    "\n",
    "        price=[int(''.join(i.text.replace('.', '').split()[:-1])) for i in soup.select('.item-price')]\n",
    "        surface_m2=[int(i) for i in re.findall(r'li\\>([0-9]+) \\<small\\>', str(soup.select('.item-tags')))]\n",
    "        rooms=[int(i.strip()[0]) for i in re.findall(r'\\d [p]', str(soup.select('.item-tags')))]\n",
    "        links=[str('https://www.pap.fr' + i.get('href')) for i in soup.select('.item-title')]\n",
    "        district=[i.text.strip()[6:8] for i in soup.select('.h1')]\n",
    "\n",
    "        # append to the dataset\n",
    "\n",
    "        df_local=pd.DataFrame(price)\n",
    "        df_local.columns=['Price']\n",
    "        df_local['N_rooms'] = rooms\n",
    "        df_local['Surface_m2'] = surface_m2\n",
    "        df_local['Link'] = links\n",
    "        df_local['District'] = district\n",
    "        df_local['Zone'] = str(zone_name)\n",
    "        \n",
    "        df5=df5.append(df_local, ignore_index = True) \n",
    "\n",
    "        # two seconds - not to get banned\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        # in case of an error - skip the page:\n",
    "\n",
    "    except:\n",
    "        print(f'Encountered a problem on page {k}, continuing to the next page...')\n",
    "        continue\n",
    "\n",
    "    # create new columns and print Done\n",
    "\n",
    "df5['Price_per_m2'] = df5.Price / df5.Surface_m2    \n",
    "\n",
    "    # drop dublicates and sort only by needed districts\n",
    "    \n",
    "print('By sorting out only for needed districts, the dataset will lose ' + str(df5.shape[0]-df5.loc[df5.District.isin(sort_list)].shape[0]) + ' out of ' + str(df5.shape[0]) + ' rows.')\n",
    "df5=df5.loc[df5.District.isin(sort_list)]\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add all dfs to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame()\n",
    "df=df.append(df1, ignore_index = True)\n",
    "df=df.append(df2, ignore_index = True)\n",
    "df=df.append(df3, ignore_index = True)\n",
    "df=df.append(df4, ignore_index = True)\n",
    "df=df.append(df5, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2E    22\n",
       "6E    13\n",
       "5E    10\n",
       "3E    10\n",
       "Name: District, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.District.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Website'] = 'PAP'\n",
    "# drop the arrondissement column"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
